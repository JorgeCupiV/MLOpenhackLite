{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Workspace and Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore, Dataset,Experiment\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.script_run_config import ScriptRunConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "#Important for Pipelines\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "dataStoreName = 'group1datastore'\n",
    "ds = ws.datastores.get(dataStoreName)\n",
    "\n",
    "# project folder\n",
    "project_folder = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provisioning compute targets for data prep and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster for Data Preparation\n",
    "clusterNameForDataPreparation = \"clusterDataPrep\"\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=clusterNameForDataPreparation)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=2, min_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, clusterNameForDataPreparation, compute_config)\n",
    "cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "# Cluster for Model Training\n",
    "clusterNameForModelTraining = \"clusterTraining\"\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=clusterNameForModelTraining)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=2,min_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, clusterNameForModelTraining, compute_config)\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a DataReference and a run_config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataReference = DataReferenceConfiguration(datastore_name=dataStoreName,\n",
    "                                           path_on_compute=\"/data\",\n",
    "                                           path_on_datastore=\"challenge5\",\n",
    "                                           mode=\"download\",\n",
    "                                           overwrite=True)\n",
    "# create a new RunConfig object\n",
    "clusterNameForDataPreparation = 'clusterDataPrep'\n",
    "\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "run_config.target = clusterNameForDataPreparation\n",
    "run_config.data_references = {'myDataStore':dataReference}\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy','scipy'])\n",
    "run_config.environment.python.conda_dependencies.add_pip_package('azureml-dataprep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train-on-amlcompute/dataprep.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from azureml.dataprep import ColumnSelector\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from azureml.core import Dataset,Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "dataset = Dataset.auto_read_files('/data/train.csv')\n",
    "\n",
    "#Registering our dataset for historic reasons\n",
    "datasetName = 'datasetRaw-Challenge5'\n",
    "description = 'This is a raw data set for Challenge 5'\n",
    "dataset = dataset.register(workspace = ws,\n",
    "                           name = datasetName,\n",
    "                           description = description,\n",
    "                           exist_ok = True)\n",
    "\n",
    "# Getting the 20% of our dataset with a randome sampling\n",
    "seed = random.randint(0, 4294967295)\n",
    "datasetAt20 = dataset.sample('simple_random', {'probability':0.2, 'seed': seed})\n",
    "\n",
    "# Registering our dataset\n",
    "datasetName = 'dataset20-Challenge5'\n",
    "description = 'This is a data set at 20% for Challenge 5'\n",
    "datasetAt20 = datasetAt20.register(workspace = ws,\n",
    "                           name = datasetName,\n",
    "                           description = description,\n",
    "                           exist_ok = False)\n",
    "\n",
    "# Getting the first 100K rows from our DatasetAt20\n",
    "datasetAt20Definition = datasetAt20.get_definition()\n",
    "datasetFirst100KRows = datasetAt20Definition.take(100000)\n",
    "\n",
    "\n",
    "# Dropping unnecesary columns\n",
    "cols_to_drop = [\"DefaultBrowsersIdentifier\",\n",
    "                \"OrganizationIdentifier\",\n",
    "                \"PuaMode\",\n",
    "                \"SmartScreen\",\n",
    "                \"Census_ProcessorClass\",\n",
    "                \"Census_InternalBatteryType\",\n",
    "                \"Census_IsFlightingInternal\",\n",
    "                \"Census_ThresholdOptIn\",\n",
    "                \"Census_IsWIMBootEnabled\",\n",
    "                \"Census_SystemVolumeTotalCapacity\"]\n",
    "datasetFirst100KRows = datasetFirst100KRows.drop_columns(cols_to_drop)\n",
    "\n",
    "# Dropping some selected columns\n",
    "column_selector = ColumnSelector(term=\".*\", use_regex=True)\n",
    "datasetFirst100KRows = datasetFirst100KRows.replace_na(column_selector)\n",
    "datasetFirst100KRows = datasetFirst100KRows.fill_nulls(column_selector, 0)\n",
    "datasetFirst100KRows = datasetFirst100KRows.fill_errors('Census_PrimaryDiskTotalCapacity', 0)\n",
    "datasetFirst100KRows = datasetFirst100KRows.clip('Census_TotalPhysicalRAM',0,16384)\n",
    "\n",
    "# Converting our datasetFirst100KRows to a Pandas Dataframe\n",
    "df = datasetFirst100KRows.to_pandas_dataframe()\n",
    "\n",
    "# Dropping more columns\n",
    "explore_df = df\n",
    "explore_df = explore_df.drop('MachineIdentifier', 1)\n",
    "cols_to_drop.append('MachineIdentifier')\n",
    "\n",
    "#Getting categorical vs non categorical values\n",
    "# Split the data into two dataframes - one for each label value\n",
    "detections_df = explore_df[(explore_df.HasDetections==1)]\n",
    "nondetections_df = explore_df[(explore_df.HasDetections==0)]\n",
    "\n",
    "# Get the numeric features\n",
    "num_cols = [\"AVProductsInstalled\",\n",
    "            \"AVProductsEnabled\",\n",
    "            \"OsBuild\",\n",
    "            \"Census_ProcessorCoreCount\",\n",
    "            \"Census_InternalBatteryNumberOfCharges\",\n",
    "            \"Census_OSBuildNumber\",\n",
    "            \"Census_OSBuildRevision\",\n",
    "            \"Census_PrimaryDiskTotalCapacity\",\n",
    "            \"Census_TotalPhysicalRAM\",\n",
    "            \"Census_InternalPrimaryDiagonalDisplaySizeInInches\",\n",
    "            \"Census_InternalPrimaryDisplayResolutionHorizontal\",\n",
    "            \"Census_InternalPrimaryDisplayResolutionVertical\"]\n",
    "\n",
    "# Get the categorical features\n",
    "cat_cols = list(detections_df.columns)\n",
    "non_cat_cols = num_cols.copy()\n",
    "non_cat_cols.append(\"HasDetections\")\n",
    "for col in non_cat_cols:\n",
    "    cat_cols.remove(col)\n",
    "\n",
    "# Using Chi-Squared to drop more columns\n",
    "alpha = 0.005\n",
    "Y = explore_df[\"HasDetections\"].astype(str)\n",
    "    \n",
    "# Categorical feature Selection\n",
    "for var in cat_cols:\n",
    "    X = explore_df[var].astype(str)\n",
    "    df_crosstab = pd.crosstab(Y,X)\n",
    "    chi2, p, dof, expected = chi2_contingency(df_crosstab)\n",
    "    if p < alpha:\n",
    "        print(\"{0} is IMPORTANT\".format(var))\n",
    "    else:\n",
    "        print(\"{0} is not important\".format(var))\n",
    "        cols_to_drop.append(var)\n",
    "\n",
    "# Use ANOVA to get the most important numeric columns\n",
    "X = explore_df[num_cols].astype(np.float)\n",
    "X.fillna(0, inplace=True)\n",
    "y = explore_df[\"HasDetections\"]\n",
    "\n",
    "# Find the 4 most important numeric columns\n",
    "X_new = SelectKBest(k=4).fit(X, y)\n",
    "\n",
    "for i in range(len(num_cols)):\n",
    "    if X_new.get_support()[i]:\n",
    "        print(\"{0} is IMPORTANT\".format(num_cols[i]))\n",
    "    else:\n",
    "        print(\"{0} is not important\".format(num_cols[i]))\n",
    "        cols_to_drop.append(num_cols[i])\n",
    "        \n",
    "# Eliminating more columns\n",
    "more_columns = ['AVProductStatesIdentifier',\n",
    "                'OsPlatformSubRelease',\n",
    "                'OsSuite',\n",
    "                'OsBuildLab',\n",
    "                'SkuEdition',\n",
    "                'SMode',\n",
    "                'Census_OSVersion',\n",
    "                'Census_OSBranch',\n",
    "                'Census_OSEdition',\n",
    "                'Census_OSSkuName',\n",
    "                'Census_OSInstallTypeName',\n",
    "                'Census_OSWUAutoUpdateOptionsName',\n",
    "                'Census_ActivationChannel',\n",
    "                'CountryIdentifier',\n",
    "                'AvSigVersionEncoded',\n",
    "                'Platform',\n",
    "                'Processor',\n",
    "                'Census_MDC2FormFactor',\n",
    "                'Census_DeviceFamily',\n",
    "                'Census_PrimaryDiskTypeName',\n",
    "                'Census_OSArchitecture',\n",
    "                'Census_GenuineStateName', \n",
    "                'Census_PowerPlatformRoleName',\n",
    "                'AvSigVersion',\n",
    "                'Census_ChassisTypeName'\n",
    "               ]\n",
    "for col in more_columns:\n",
    "    cols_to_drop.append(col)\n",
    "    \n",
    "datasetCleaner = datasetFirst100KRows.drop_columns(cols_to_drop)\n",
    "\n",
    "# One hot encoding\n",
    "datasetOneHotEncoded = datasetCleaner.label_encode('EngineVersion','EngineVersionEncoded')\n",
    "datasetOneHotEncoded = datasetOneHotEncoded.label_encode('AppVersion','AppVersionEncoded')\n",
    "datasetOneHotEncoded = datasetOneHotEncoded.label_encode('Census_FlightRing','Census_FlightRingEncoded')\n",
    "cols_to_drop = [\"EngineVersion\",\n",
    "                \"AppVersion\",\n",
    "                \"CensusFlightRing\"]\n",
    "datasetCleaned = datasetOneHotEncoded.drop_columns(cols_to_drop)\n",
    "\n",
    "# Normalization\n",
    "datasetNormalized = datasetCleaned.min_max_scale('AVProductsInstalled',0,5)\n",
    "datasetNormalized = datasetNormalized.min_max_scale('Census_TotalPhysicalRAM',512,16384)\n",
    "\n",
    "# Updating definition\n",
    "datasetAt20.update_definition(datasetNormalized,'Applied transformations for data prep')\n",
    "\n",
    "df = datasetAt20.to_pandas_dataframe()\n",
    "df.to_csv('preppedChallenge1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataReference = DataReference(datastore=ds,\n",
    "                                 path_on_compute=\"/data\",\n",
    "                                 path_on_datastore=\"challenge5\",\n",
    "                                 mode=\"download\",\n",
    "                                 overwrite=True)\n",
    "\n",
    "trainStep = PythonScriptStep(name=\"trainStep\",\n",
    "                             script_name=\"train-on-amlcompute/dataprep.py\", \n",
    "                             compute_target=cpu_cluster, \n",
    "                             source_directory=project_folder,\n",
    "                             runconfig=run_config,\n",
    "                             inputs=[newDataReference],\n",
    "                             allow_reuse=True)\n",
    "print(\"Train step created\")\n",
    "\n",
    "#Definition of steps\n",
    "steps = [trainStep]\n",
    "\n",
    "# Definition of pipelines\n",
    "myPipeline = Pipeline(workspace=ws, steps=steps)\n",
    "print (\"Pipeline is built\")\n",
    "\n",
    "myPipeline.validate()\n",
    "print(\"Pipeline validation complete\")\n",
    "\n",
    "myPipelineRun = Experiment(ws, 'Challenge5JorgeExperiment').submit(myPipeline, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(myPipelineRun).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
