{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.script_run_config import ScriptRunConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "experiment_name = '<ExperimentName>'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a local folder\n",
    "!rm -rf ./train-on-amlcompute\n",
    "project_folder = './train-on-amlcompute'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "os.mknod(\"./train-on-amlcompute/train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provisioning a compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"<ClusterName>\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train-on-amlcompute/train.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "\n",
    "# Create an in-memory Dataset on your local machine\n",
    "df = pd.read_csv(\"/data/prepped_train.csv\")\n",
    "\n",
    "# Getting labels and features from our dataset\n",
    "labels =  df['HasDetections']\n",
    "features = df.drop(columns=['HasDetections'])\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(features, labels, test_size=0.30)\n",
    "\n",
    "# Get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "# Train a classifier\n",
    "# Create a pipeline (in case we want to add transformations later)\n",
    "pipeline = Pipeline([('classify', RandomForestClassifier()),\n",
    "                        ])\n",
    "# Use the pipeline to fit a model to the training data\n",
    "print(\"Training model...\")\n",
    "classifier = pipeline.fit(xTrain, yTrain)\n",
    "print('Classifier trained!')\n",
    "\n",
    "# Evaluate classifier\n",
    "classes = ['Has detections','Does not have detections']\n",
    "print('Calculating classifier metrics...')\n",
    "predictions = classifier.predict(xTest)\n",
    "print(metrics.classification_report(yTest, predictions, target_names=classes))\n",
    "\n",
    "acc = metrics.accuracy_score(yTest, predictions)\n",
    "print('Accuracy:' + str(acc))\n",
    "\n",
    "# Saving the accuracy as a metric to our workspace\n",
    "run.log('accuracy', np.float(acc))\n",
    "\n",
    "print(\"\\n Confusion Matrix:\")\n",
    "cm = confusion_matrix(yTest, np.round(predictions, 0))\n",
    "print(cm)\n",
    "\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=classifier, filename='outputs/challenge3model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring our cluster and running it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "\n",
    "dataReference = DataReferenceConfiguration(datastore_name=\"<YourDataStoreName>\",\n",
    "                                           path_on_compute=\"/data\",\n",
    "                                           path_on_datastore=\"prepped\",\n",
    "                                           mode=\"download\",\n",
    "                                           overwrite=False)\n",
    "\n",
    "# create a new RunConfig object\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute target created in previous step\n",
    "run_config.target = cpu_cluster.name\n",
    "\n",
    "run_config.data_references = {'myDataStore':dataReference}\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn','pandas'])\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder, \n",
    "                      script='train.py',\n",
    "                      run_config=run_config)\n",
    "\n",
    "run = experiment.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Estimators and Hyperdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-sdk[notebooks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.hyperdrive import HyperDriveRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.hyperdrive import RandomParameterSampling\n",
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "from azureml.train.hyperdrive import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import normal, uniform, choice\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "import os \n",
    "\n",
    "\n",
    "os.makedirs('./train-on-amlcompute', exist_ok=True)\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': '/data',\n",
    "    '--regularization': 0.8\n",
    "}\n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "        \"learning_rate\": normal(10, 3),\n",
    "        \"keep_probability\": uniform(0.05, 0.1),\n",
    "        \"batch_size\": choice(16, 32, 64, 128)\n",
    "    }\n",
    ")\n",
    "\n",
    "dataReference = DataReferenceConfiguration(datastore_name=\"<YourDataStoreName>\",\n",
    "                                           path_on_compute=\"/data\",\n",
    "                                           path_on_datastore=\"prepped\",\n",
    "                                           mode=\"download\",\n",
    "                                           overwrite=False)\n",
    "\n",
    "cpu_cluster.data_references = {'myDataStore':dataReference}\n",
    "\n",
    "estimator = Estimator(source_directory='./train-on-amlcompute',\n",
    "                   script_params=script_params,\n",
    "                   compute_target=cpu_cluster,\n",
    "                   entry_script='train.py',\n",
    "                   conda_packages=['scikit-learn','pandas'])\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval=1, delay_evaluation=5)\n",
    "\n",
    "primary_metric_name=\"accuracy\",\n",
    "primary_metric_goal=PrimaryMetricGoal.MAXIMIZE\n",
    "\n",
    "hyperdrive_run_config = HyperDriveRunConfig(estimator=estimator,\n",
    "                          hyperparameter_sampling=param_sampling, \n",
    "                          policy=early_termination_policy,\n",
    "                          primary_metric_name=\"accuracy\", \n",
    "                          primary_metric_goal=primary_metric_goal,\n",
    "                          max_total_runs=100,\n",
    "                          max_concurrent_runs=4)\n",
    "\n",
    "\n",
    "experiment_name = '<YourNewExperimentName>'\n",
    "experiment = Experiment(workspace = ws, name = experiment_name)\n",
    "\n",
    "hyperdrive_run = experiment.submit(hyperdrive_run_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
